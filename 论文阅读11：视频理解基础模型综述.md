# **研究背景**

1. 研究问题：这篇文章要解决的问题是如何通过视频基础模型（ViFMs）来学习通用的视频理解表示。视频基础模型旨在从视频数据中捕获稳健和通用的特征，以应用于各种视频理解任务。
2. 研究难点：该问题的研究难点包括：视频数据的复杂性、时间维度的影响以及数据量的庞大。此外，现有的视频分析方法大多基于处理单独的帧或使用标准图像分析技术，缺乏对时间维度的充分考虑。
3. 相关工作：该问题的研究相关工作有：早期的视频分析方法主要基于处理单独的帧或使用标准图像分析技术；更高级的技术如3D卷积、循环网络、光流和变换器被开发出来专门用于视频处理；此外，多模态增强视频理解的研究也得到了广泛探索。

# **前言**

## **视频理解任务**

![image-20241127193728942](https://raw.githubusercontent.com/chongzicbo/images/main/picgo/image-20241127193728942.png)

### **视频内容理解**

#### **抽象理解任务**

- 分类或识别：该任务是将视频分配一个类别
- 检索：该任务涉及找到包含特定动作、物体或场景的视频。评估此任务的常见指标是K次召回率(R@K)

#### **时间理解任务**

- 时间动作定位（TAL）。TAL[304]旨在精确指出视频中特定动作发生的确切时刻。
- 细粒度分类。这项任务扩展了长格式视频的分类任务
- 参考视频分割(RVS)。该任务的评估指标是平均AP和平均IoU。

#### **时空理解任务**

- 时空动作定位（SAL）。SAL旨在找到视频中特定动作展开的“何时”和“何地”。
- 跟踪。该任务旨在识别并跟随视频中物体运动的轨迹。
- 视频目标检测(VOD)。VOD通常旨在检测视频流中的对象。视频通常包含大量冗余的时间信息。这有助于检测器在当前帧中检测到对象，并预测其在后续帧中的位置

#### **描述性理解任务**

- 视频问答(VQA)。VQA基于视觉信息和可能的文本查询回答关于视频内容的问题。

- - i) 多选题（MC）
  - ii) 开放性（OE）：OE-VQA回答主观、创造性和逻辑性问题
  - iii) 长篇（LF）

- 全面的解释，理解视频内容，推理时间，并适应不同的问题类型。

- 视频字幕制作。视频字幕制作生成视频内容的文本描述

### **视频内容生成与操纵**

#### **视频预测**

- (a) 视频未来预测（VFP）：VFP根据给定的可变长度输入视频预测未来的帧。
- 文本到视频（T2V）生成
- 视频修复/修复。
- 视频风格化

## **架构与损失函数**

### **架构**

![image-20241127193916621](https://raw.githubusercontent.com/chongzicbo/images/main/picgo/image-20241127193916621.png)

#### **单模态**

专注于单一模态（例如视频），通常采用生成性目标，如掩码重建。它们通常遵循一个编码器-解码器框架，其中编码器从输入视频中提取特征，解码器重建被遮挡或缺失的部分。训练过程由一个重建损失函数指导，该函数衡量原始视频和重建视频之间的差异。

#### **多模态**

通常依赖于对比学习目标。它们通常使用仅编码器的网络，可能为每种模态分别使用独立的编码器。在这里，重点是学习能够捕捉不同模态之间关系的表示。在训练过程中使用对比损失函数（VTC）来结合相似的表示并分离不相似的表示。在多模态模型中，不同模态的编码风格可以进一步分类为三类：

- 联合编码器：使用单个编码器同时处理所有模态。
- 双/多编码器：为每个模态使用单独的编码器
- 混合编码器：提供了联合编码器和单独编码器之间的折中方案。它首先使用轻量级编码器从各个模态提取初始特征。然后这些特征被组合并通过共享编码器处理，最后达到最终的损失函数。单模态和多模态架构通常利用变换器块作为它们的基本构建模块

### **损失函数**

视频基础模型（ViFMs）依赖于预训练特定目标（损失函数）来学习有效的表示。这些目标可以大致分为两大类：生成性和判别性。

#### **判别式**

- 视频-文本对比(VTC)：将相似的表示组合在一起，将不同的表示分开

- 视频-文本匹配(VTM)：旨在最大化给定视频-文本对之间的匹配分数

- 其它：

- - 动词聚焦对比(VFC)[208]专注于细粒度的动词对齐
  - 视频-文本联合(VTJ)[190]从视频和文本中学习联合表示（参考图4(d)）
  - 多模态时间对比(MTC)[268]将对比学习扩展到其他模态
  - 视频剪辑对比(VCC)[270]利用视频剪辑之间的对比学习
  - 三模态对齐(TMA)[115]以实现同时跨模态对齐和融合
  - 全模态视频字幕对比(OM-VCC)[45]
  - 视频字幕匹配(OM-VCM)[45]损失
  - 视频-音频对比(VAC)[3, 94]和注意力引导对比(AGC)[227]目标。

#### **生成式**

涵盖了各种专注于在视频数据中重建掩码信息的目标

- 文本数据的掩码语言建模（MLM）
- 视频的掩码视频建模（MVM）
- 一般信号的掩码信号/数据建模
- 视频帧的掩码帧建模（FMF）
- 以及图像的掩码图像建模（MIM）

#### **其它目标**

多模态架构生成方法。这些包括：自回归训练目标，如语言建模（LM）、前缀LM和下一个（Image/Motion/Text）令牌生成，它基于当前元素预测序列中的下一个元素。标题损失（Captioning Loss）基于过去的视频和文本预测下一个令牌。视觉音频连续性（VAST）通过全模态视频标题生成（OM-VCG）损失修改了这个概念。音频视频延续（AVCont）从音频、图像补全和补全预测下一个帧，文本到图像/视频生成（Text-to-Image/Video Generation），视频到文本完成（VTC），以及帧预测（Frame Prediction）。

特定任务的选项。提示实体建模（PEM）专注于细粒度的区域-实体对齐和动作理解。多粒度对齐（MGA）将视觉概念（对象）与文本描述对齐，而多粒度定位（MGL）根据文本描述在图像中定位这些概念。多选择建模（MCM）改进了模态对齐和表示学习。蒸馏损失[33, 351]，其中学生网络模仿更强大教师网络的表示，是另一种常见的ViFM预训练目标，用于知识迁移，特别是用于训练轻量级学生网络。

### **训练策略**

#### **自监督预训练数据集**

- 单模态。在单模态中，大规模模型主要使用动作识别数据集进行自监督预训练。K400[136]、K600[28]、K700[29]、SomethingSomethingV1(SSv1)[96]和SomethingSomethingV2(SSv2)[96]是用于此类情况的少数数据集。
- 多模态。用于训练多模态视频基础模型的视频-文本数据集列如下：WebVid-2M[13]、HowTo100M[201]、EpicKitchen[60]、Flinstones[103]、Mugen[104]。由于我们在视频领域有限的多模态（视觉-语言）数据集数量，为了满足这一需求，一些基础模型[43, 81, 82, 154, 273, 291]使用了图像-文本数据集，如CC3M[258]和CC12M[37]，以及SBU Captions[221]进行多模态视频基础模型的预训练。此外，少数基础模型[45, 268, 326]进一步整理了自己的数据集，如HD-VILA-100M[326]和LF-VILA-8M[326]，以及VAST-27M[45]，以便为多模态预训练提供多样化和大规模的数据集。

#### **半监督预训练数据集**

##### **组合数据集**

视觉领域的定位任务不仅仅通过自监督学习来解决。在这种情况下，准备大规模标注数据也是一个繁琐的任务。因此，不同的模型使用数据集的组合：Object365[257]、OpenImages[149]和COCO[176]用于对象检测；RefCOCO[340]、RefCOCO+[340]、RefCOCOg[213]和VisualGenome[144]用于视觉定位；LVIS[102]、BDD[337]用于跟踪；YTVIS19[332]、YTVIS21[217]、RVOS[255]和OVIS[235]用于视频分割。

##### **伪标记数据集**

大规模标注数据的需求在计算机视觉中仍然是一个挑战。最近的趋势涉及利用一些强大的教师模型来提供与不同视觉任务相关的高质量标签。这种方法由GRIT[229]首创，它利用教师模型为定位任务生成标签。在此成功之后，SAM[140]提出了一种主动学习方法，专门为分割任务生成高质量标注数据。这种方法产生了一个非常大的数据集，SA-1B[140]，包含10亿个高质量注释。同样地，Distill VLM[361]利用教师模型为现有的视频数据集生成字幕，如VideoCC[214]和InternVid[302]。这个过程创建了两个新的伪字幕数据集：VideoCC+和InternVid+

#### **部署用于视频理解的基模型**

![image-20241127193952065](https://raw.githubusercontent.com/chongzicbo/images/main/picgo/image-20241127193952065.png)

##### **微调**

##### **适配器**

##### **提示调优**

# **基于图像的视频基础模型**

## **后训练扩展**

a）后预训练（在图像模型上进行进一步微调），b）适配器（在预训练的图像模型内部引入少量可训练层），c）提示式微调（在预训练模型的输入处引入可训练参数），d）混合（使用上述技术的组合）。这些方法描述了两种突出的架构模式，即：ED（编码器-解码器）和DE（双编码器）。

![image-20241127194116898](https://raw.githubusercontent.com/chongzicbo/images/main/picgo/image-20241127194116898.png)

## **使用适配器扩展**

使用轻量级适配器层进行时间建模，而不是微调整个预训练的图像模型。这种方法适用于视频任务，无需额外的预训练。

## **提示式微调的扩展**

![image-20241127194142479](https://raw.githubusercontent.com/chongzicbo/images/main/picgo/image-20241127194142479.png)

与适配器类似，提示式微调通过仅微调几个额外的参数来提高效率。

## **混合方法扩展**

# **基于视频的模型**

![image-20241127194220007](https://raw.githubusercontent.com/chongzicbo/images/main/picgo/image-20241127194220007.png)

# **通用基础模型**

![image-20241127194233964](https://raw.githubusercontent.com/chongzicbo/images/main/picgo/image-20241127194233964.png)

[Foundation Models for Video Understanding: A Survey (arxiv.org)](https://arxiv.org/pdf/2405.03770)