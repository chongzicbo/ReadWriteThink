![img](https://pic2.zhimg.com/80/v2-17eded5face0e3a918748ee945294a85_720w.webp)

今天主要介绍下在大模型中常用的激活函数。

# ReLU(Rectified Linear Unit)

![img](https://img-blog.csdnimg.cn/20210106110219329.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MTkyOTUyNA==,size_16,color_FFFFFF,t_70#pic_center)

ReLU，全称为：Rectified Linear Unit，是一种[人工神经网络](https://so.csdn.net/so/search?q=人工神经网络&spm=1001.2101.3001.7020)中常用的激活函数，通常意义下，其指代数学中的斜坡函数，即
$$
f(x) =max(0,x)
$$
而在神经网络中，ReLU函数作为神经元的激活函数，为神经元在线性变换 $w^T x+b$之后的非线性输出结果。换言之，对于进入神经元的来自上一层神经网络的输入向量$x$ ,使用ReLU函数的神经元会输出
$$
max(0,w^Tx+b)
$$

* 优势

  相比于传统的神经网络激活函数，诸如逻辑函数（Logistic sigmoid）和tanh等双曲函数，ReLU函数有着以下几方面的优势：

  * 仿生物学原理：相关大脑方面的研究表明生物神经元的讯息编码通常是比较分散及稀疏的。通常情况下，大脑中在同一时间大概只有1%-4%的神经元处于活跃状态。使用线性修正以及正规化;（regularization）可以对机器神经网络中神经元的活跃度（即输出为正值）进行调试；相比之下，逻辑函数在输入为0时达到 0.5，即已经是半饱和的稳定状态，不够符合实际生物学对模拟神经网络的期望。不过需要指出的是，一般情况下，在一个使用ReLU的神经网络中大概有50%的神经元处于激活态。

  * 更加有效率的梯度下降以及反向传播：避免了梯度爆炸和梯度消失问题;

  * 简化计算过程：没有了其他复杂激活函数中诸如指数函数的影响；同时活跃度的分散性使得神经网络整体计算成本下降.