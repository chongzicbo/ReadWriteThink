![img](https://pic2.zhimg.com/80/v2-17eded5face0e3a918748ee945294a85_720w.webp)

今天主要介绍下在大模型中常用的激活函数。

# ReLU(Rectified Linear Unit)

![img](https://img-blog.csdnimg.cn/20210106110219329.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MTkyOTUyNA==,size_16,color_FFFFFF,t_70#pic_center)

ReLU，全称为：Rectified Linear Unit，是一种[人工神经网络](https://so.csdn.net/so/search?q=人工神经网络&spm=1001.2101.3001.7020)中常用的激活函数，通常意义下，其指代数学中的斜坡函数，即
$$
f(x) =max(0,x)
$$
而在神经网络中，ReLU函数作为神经元的激活函数，为神经元在线性变换 $w^T x+b$之后的非线性输出结果。换言之，对于进入神经元的来自上一层神经网络的输入向量$x$ ,使用ReLU函数的神经元会输出
$$
max(0,w^Tx+b)
$$

* 优势

  相比于传统的神经网络激活函数，诸如逻辑函数（Logistic sigmoid）和tanh等双曲函数，ReLU函数有着以下几方面的优势：

  * 仿生物学原理：相关大脑方面的研究表明生物神经元的讯息编码通常是比较分散及稀疏的。通常情况下，大脑中在同一时间大概只有1%-4%的神经元处于活跃状态。使用线性修正以及正规化;（regularization）可以对机器神经网络中神经元的活跃度（即输出为正值）进行调试；相比之下，逻辑函数在输入为0时达到 0.5，即已经是半饱和的稳定状态，不够符合实际生物学对模拟神经网络的期望。不过需要指出的是，一般情况下，在一个使用ReLU的神经网络中大概有50%的神经元处于激活态。

  * 更加有效率的梯度下降以及反向传播：避免了梯度爆炸和梯度消失问题;

  * 简化计算过程：没有了其他复杂激活函数中诸如指数函数的影响；同时活跃度的分散性使得神经网络整体计算成本下降.

# GELU

研究者表明，收到dropout、ReLU等机制的影响，它们都希望将不重要的激活信息规整为0，我们可以理解为，对于输入的值，我们根据它的情况乘上1或者0，更数学一点的描述是，对于每一个输入x，其服从标准的正太分布 $N(0,1)$ ,它会乘上一个伯努利分布$Bernoulli(\phi(x))$  ,其中，$\phi(x)=P(X \leq x)$。

随着x的降低，它被归零的概率会升高，对于ReLU来说，这个界限就是0，输入少于零就会被归为0，这一类激活函数，不仅保留了概率性，同时也保留了对输入的依赖性。

GELU ：高斯误差线性单元激活函数，在最近的Transformer模型（谷歌的BERT和OpenAI的GPT-2）中得到了应用,GELU的论文来自2016年，但是最近才引起关注，这种激活函数的形式为：
$$
xP(X \leq x) =x \phi(x)
$$
其中$\phi (x)$指的是x的正态分布的累积分布，完整形式如下：
$$
x P(X\leq x)=x\int_{-\infty}^{x}{\frac{e^{-{\frac{(x-\mu)^{2}}{\sqrt{2}}}}}{\sqrt{2}\pi\sigma}}\,\mathrm{d}X
$$
计算结果约为：  
$$
0.5x(1+t a n h[\textstyle{\sqrt{\frac{2}{\pi}}}(x+0.044715x^{3})])
$$
或者表示为：
$$
x\sigma(1.702x)
$$
可以看看GELU长啥样

![img](https://pic1.zhimg.com/80/v2-3f1115a021c4201c79a5c596a8eac0ec_720w.webp)

# SwiGLU

Swish Gated Linear Unit是一种新型的激活函数，它结合了Swish激活函数和Gated Lineal Unit（GLU）的优点。

## Swish

Swish是谷歌研究人员于2017年提出的一种非单调激活函数。Swish的定义如下：
$$
S w i s h=x\cdot s i g m o i d(\beta x)
$$
其中$\beta$是可训练的参数。Swish已被证明在许多应用程序中比ReLU性能更好，尤其是在深度网络中。Swish的主要优点是它比ReLU更平滑，这可以带来更好的优化和更快的收敛。

## GLU

门控线性单元（GLU）是微软研究人员于2016年提出的一种激活函数。GLU定义如下：
$$
GLU(x)=x \cdot sigmoid(Wx+b)
$$
GLU与Swish的相似之处在于它将线性函数与非线性函数相结合。然而，在GLU中，线性函数由S形激活函数门控。GLU已被证明在自然语言处理任务中特别有效，如语言建模和机器翻译。

SwiGLU是Swish和GLU激活功能的组合。SwiGLU的定义如下：
$$
SwiGLU(x)=x \cdot sigmoid(\beta \cdot x)+(1-sigmoid(\beta \cdot x)) \cdot (Wx+b)
$$
在SwiGLU中，Swish函数用于gate GLU的线性函数。这使得SwiGLU既能抓住Swish和GLU的优点，又能克服它们各自的缺点。

SwiGLU已被证明在各种任务中都优于Swish和GLU，包括图像分类、语言建模和机器翻译。

![img](https://blogger.googleusercontent.com/img/a/AVvXsEhooRvmBbtLr_iUtJMA3ZJOB53ZIx4R5Iiv4jAFhc7Qt3dlCsrxZ0_2WG8ZBoQu9E-JvUzo5ytdT3YEt8T0wNjcuBcHCzjrcmSPjhbCv9L8mHTZu4HJY36iwYE9jhUgnlZ4tnXkg2CBmz-IOR_dqvdp545_g2vEVMn8PAs_UjsCAKyDbNfL3HbeMxyz)

SwiGLU与其他激活函数相比的主要优势是：

* 平滑性：SwiGLU比ReLU更平滑，这可以带来更好的优化和更快的收敛。
* 非单调性：SwiGLU是非单调的，这使它能够捕捉输入和输出之间复杂的非线性关系。
* 门控：SwiGLU使用门控机制，使其能够根据收到的输入选择性地激活神经元。这有助于减少过拟合并提高泛化能力。
* 性能：SwiGLU已被证明在各种任务中优于其他激活功能，包括Swish和GLU。





# GEGLU

论文链接：[GLU Variants Improve Transformer](https://arxiv.org/abs/2002.05202v1)

GEGLU是使用`GELU`作为激活函数的GLU变体。GEGLU的数学表达式如下所示：
$$
G E G L U(x)=G E L U(x W+b)\otimes(x V+c)
$$




【1】[深入理解ReLU函数（ReLU函数的可解释性）_Kanny广小隶的博客-CSDN博客](https://blog.csdn.net/weixin_41929524/article/details/112253138)

【2】[GELU激活函数 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/394465965)

【3】[SwiGLU Activation Function (ai-contentlab.com)](https://www.ai-contentlab.com/2023/03/swishglu-activation-function.html)

【4】[激活函数总结（八）：基于Gate mechanism机制的激活函数补充(GLU、SwiGLU、GTU、Bilinear、ReGLU、GEGLU)_sjx_alo的博客-CSDN博客](https://blog.csdn.net/qq_36758270/article/details/132174106)