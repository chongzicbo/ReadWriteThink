# 摘要

我们介绍了YOLO9000，这是一个最先进的实时物体检测系统，可以检测9000多个物体类别。首先，我们对YOLO检测方法提出了各种改进，既新颖又借鉴了先前的工作。改进后的模型YOLOv2在PASCAL VOC和COCO等标准检测任务中是最先进的。使用一种新颖的多尺度训练方法，相同的YOLOv2模型可以在不同的大小下运行，从而在速度和精度之间实现轻松的折衷。在67 FPS的速度下，YOLOv2在VOC 2007上获得76.8mAP。在40 FPS的速度下，YOLOv2获得78.6mAP时，性能优于最先进的方法，如使用ResNet和SSD的Faster RCNN，同时运行速度仍明显更快。最后，我们提出了一种在目标检测和分类方面进行联合训练的方法。使用这种方法，我们在COCO检测数据集和ImageNet分类数据集上同时训练YOLO9000。我们的联合训练使YOLO9000能够预测没有标记检测数据的对象类的检测。我们在ImageNet检测任务上验证了我们的方法。YOLO9000在ImageNet检测验证集上获得19.7mAP，尽管200个类别中只有44个类别的检测数据。在不在COCO的156个类别中，YOLO9000获得16.0mAP。但YOLO可以检测到200多个类；它预测了9000多个不同对象类别的检测。它仍然实时运行。

# 前言

通用物体检测应该快速、准确，并且能够识别各种各样的物体。自从引入神经网络以来，检测框架变得越来越快速和准确。然而，大多数检测方法仍然局限于一小部分对象。与分类和标记等其他任务的数据集相比，当前的对象检测数据集是有限的。最常见的检测数据集包含数千到数十万张带有数十到数百个标签的图像。分类数据集有数百万张图像，具有数万或数十万个类别。

我们希望检测能够扩展到物体分类的级别。然而，标记图像用于检测远比标记图像用于分类昂贵（标签通常是用户免费提供的）。因此，在不久的将来，我们不可能看到与分类数据相同规模的检测数据集。

我们提出了一种新的方法来利用我们已经拥有的大量分类数据，并使用它来扩大当前检测系统的范围。我们的方法使用物体分类的分层视图，使我们能够将不同的数据集组合在一起。我们还提出了一种联合训练算法，允许我们在检测和分类数据上训练物体检测器。我们的方法利用标记的检测图像来学习精确定位物体，同时使用分类图像来增加其词汇表和鲁棒性。

使用这种方法，我们训练了YOLO9000，这是一种实时物体检测器，可以检测9000多种不同的物体类别。首先，我们改进了基础YOLO检测系统，也就是YOLOv2，这是一种最先进的实时检测器。然后，我们使用数据集组合方法和联合训练算法在ImageNet的9000多个类以及COCO的检测数据上训练模型。

我们所有的代码和预训练模型都可以在线访问http://pjreddie.com/yolo9000/.



# 改进

## YOLOv1算法缺点：

(1) 在物体定位方面（localization）不够准确

(2) 难以找到图片中的所有物体，召回率（recall）较低

(3) 检测小目标和密集目标性能比较差

(4) 虽然速度快，但是map准确度比较低

针对YOLO V1的缺点，YOLOv2提出了好几种改进策略来提升YOLO模型的效果。

![img](https://raw.githubusercontent.com/chongzicbo/images/main/picgo/v2-44e3f66b29f9cbb112c52b7f791dd400_r.jpg)

## 加入Batch Normalization层

在YOLOv2中，**每个卷积层后面添加了Batch Normalization层**，不再使用dropout。实验证明添加了BN层可以提高2%的mAP。

Batch Normalization有助于解决反向传播过程中的梯度消失和梯度爆炸问题，降低对一些超参数（比如学习率、网络参数的大小范围、激活函数的选择）的敏感性，可以提升模型收敛速度，而且可以起到一定正则化效果，降低模型的过拟合。

## High Resolution Classifier（高分辨率图像分类器）

所有最先进的检测方法都使用在ImageNet上预先训练的分类器。从AlexNet开始，大多数分类器对小于256×256的输入图像进行操作。原始YOLO以224×224训练分类器网络，并将检测分辨率提高到448。原始YOLO以224×224训练分类器网络，并将检测分辨率提高到448。这样切换对模型性能有一定影响。

所以YOLO2在采用 224×224 图像进行分类模型预训练后，再采用 448×448 的高分辨率样本对分类模型进行微调（10个epoch），使网络特征逐渐适应 448×448 的分辨率。然后再使用 448×448 的检测样本进行训练，缓解了分辨率突然切换造成的影响。

- mAP提升了约4%。

## Convolution with Anchor boxes(使用先验框)

在YOLOv1中，输入图片被划分为 7×7 的网格grid cell，每个网格预测2个边界框bounding box，采用**全连接层直接对边界框进行预测** $(x,y,w,ℎ)$ ，其中边界框的宽与高是相对整张图片的大小的，而由于各个图片中存在不同尺度和长宽比的物体，YOLOv1在训练过程中学习适应不同物体的形状比较困难，精准定位较难。这也导致YOLOv1在精确定位方面表现较差.

借鉴Faster R-CNN的做法，YOLOv2移除了YOLOv1中的全连接层而采用了卷积核anchor boxes来预测边界框。通过在每个cell预先设定一组不同大小和宽高比的边框，来覆盖整个图像的不同位置和多种尺度。同时为了使检测实用的特征图分辨率更高，去掉了网络中的一个pooling层。并且网络输入图像尺寸变为416×416，而不是原来的448×448。

使用anchor boxes之后，YOLOv2的mAP有稍微下降。YOLOv1只能预测98个边界框，而YOLOv2使用anchor boxes之后可以预测上千个边界框,召回率大大提升，由原来的81%升至88%。



## Dimension Clusters（聚类提取先验框）

提出anchors后第一个问题是如何确定先验框的尺寸。YOLOv2的做法是**对训练集中标注的边框进行K-mean聚类分析**，以寻找尽可能匹配样本的边框尺寸。由于设置先验框的主要目的是为了使得预测框与ground truth的IOU更好，所以聚类分析时选用box与聚类中心box之间的IOU值作为距离指标。
$$
d(b o x,c e n t r o i d)=1-I O U(b o x,c e n t r o i d)
$$
centroid是聚类时被选作中心的边框，box就是其他边框，d是两者之间的“距离”，IOU越大，“距离”越近。

![img](https://raw.githubusercontent.com/chongzicbo/images/main/picgo/v2-d1d8b71299bdeddb13243bc537a34f9a_1440w.webp)

 上图左边是选择不同的聚类k值情况下，得到的k个centroid边框，计算样本中标注的边框与各centroid的Avg IOU。显然，边框数k越多，Avg IOU越大。YOLO2选择k=5作为边框数量与IOU的折中。对比手工选择的先验框，使用5个聚类框即可达到61 Avg IOU，相当于9个手工设置的先验框60.9 Avg IOU。
    上图右边显示了5种聚类得到的先验框，VOC和COCO数据集略有差异，不过都有较多的瘦高形边框。

## Direct location prediction(位置预测)

使用anchor box时会遇到第二个问题：模型不稳定，尤其在训练早期。大多数不稳定性来自于预测box的$（x，y）$位置。在RPN网络中，网络预测值$t_x$和$t_y$，并且$（x，y）$中心坐标计算为
$$
x=(t_x * w_a)-x_a
$$

$$
y=(t_y * h_a)-y_a
$$

 其中$ (x,y) $为边界框的实际中心位置，需要预测的坐标偏移值为 $(t_x,t_y) $,先验框的尺寸为 $w_a,h_a$以及中心坐标 $(x_a,y_a) $(特征图每个位置的中心点)。由于 $(t_a,t_b)$ 取值没有任何约束，因此预测边框的中心可能出现在任何位置，训练早期不容易稳定。

所以，YOLOv2弃用了这种预测方式，而是沿用YOLOv1的方法，就是**预测边界框中心点相对于对应cell左上角位置的相对偏移值**，为了将边界框中心点约束在当前cell中，使用sigmoid函数处理偏移值，这样预测的偏移值在(0,1)范围内（每个cell的尺度看做1）。总结来看，根据边界框预测的4个offsets $t_x,t_y,t_w,t_h$ ,可以按如下公式计算出边界框实际位置和大小：
$$
b_{x}=\sigma(t_{x})+c_{x} \\
b_{y}=\sigma(t_{y})+c_{y} \\
b_{w}=p_{w}e^{t_{w}} \\
b_{h}=p_{h}e^{t_{h}} \\
P r(o b j e c t)\ {^{*}I O U(b,\,o b j e c t)}=\delta(t_{o})
$$
其中，$b_x,b_y,b_w,b_h$是预测边框的中心和宽高。$Pr(object)*IOU(b,object)$是预测边框的置信度，YOLOv1是直接预测置信度的值，这里对预测参数$t_o$进行δ变换后作为置信度的值。$C_x,C_y$是当前网格左上角到图像左上角的距离，要先将网格大小归一化，即令一个网格的宽=1,高=1。$p_w,P_h$是anchor的宽和高。$\delta$是sigmoid函数。$t_x,t_y,t_w,t_h,t_o$是要学习的参数，分别用于预测边框的中心和宽高，以及置信度。
因为使用了限制让数值变得参数化，也让网络更容易学习、更稳定。

## Fine-Grained Features（passthrough层检测细粒度特征）

YOLOv2的输入图片大小是$416*416$，经过5次2*2 maxpooling之后得到$13 * 13$大小的特征图，并以此特征图采用卷积做预测。虽然13*13的feature map对于预测大的object以及足够了，但是对于预测小的object就不一定有效。这里主要是添加了一个层：**passthrough layer**。这个层的作用就是**将前面一层的26\*26的feature map和本层的13\*13的feature map进行连接**，有点像ResNet。

## Multi-Scale Training（多尺度图像训练）

​    由于YOLOv2模型中只有卷积层和池化层，所以YOLOv2的输入可以不限于416×416大小的图片。为了增强模型的鲁棒性，YOLOv2采用了多尺度输入训练策略，具体来说就是在训练过程中每间隔一定的iterations之后改变模型的输入图片大小。由于YOLOv2的下采样总步长为32，输入图片大小选择一系列为32倍数的值：{320，352，…，608}，输入图片最小为320×320，此时对应的特征图大小为10×10，而输入图片最大为608*608,对应的特征图大小为19×19。在训练过程，每隔10个iterations随机选择一种输入图片大小，然后只需要修改对最后检测层的处理就可以重新训练。 这样训练出来的模型可以预测多个尺度的物体。并且，输入图片的尺度越大则精度越高，尺度越小则速度越快，因此YOLOv2多尺度训练出的模型可以适应多种不同场景的要求。

![在这里插入图片描述](https://raw.githubusercontent.com/chongzicbo/images/main/picgo/20210302004638950.jpg)
    采用Multi-Scale Training策略，YOLOv2可以适应不同大小的图片，并且预测出很好的结果。在测试时，YOLOv2可以采用不同大小的图片作为输入，在VOC 2007数据集上的效果如下图所示。可以看到采用较小分辨率时，YOLOv2的mAP值略低，但是速度更快，而采用高分辨输入时，mAP值更高，但是速度略有下降，对于544*544，mAP高达78.6%。注意，这只是测试时输入图片大小不同，而实际上用的是同一个模型（采用Multi-Scale Training训练）。

![在这里插入图片描述](https://raw.githubusercontent.com/chongzicbo/images/main/picgo/20210302004841404.png)