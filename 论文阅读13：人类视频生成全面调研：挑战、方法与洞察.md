# **1.引言**

### **研究问题**

如何利用生成模型（如变分自编码器VAE、生成对抗网络GAN和扩散模型）根据文本、音频和姿态等控制条件合成自然的2D人类视频序列。

### **挑战性**

- 人类在时间序列上的外观一致性是这项任务中的一个重大障碍
- 在合成视频中人们敏感的身体变形很难避免，即手指异常
- 人体运动视频的复杂性不仅限于建模面部；它还涉及准确建模身体运动，并保持与身体部位的背景一致性和和谐。
- 人体运动生成的需求通常包括一个条件作为背景，例如文本描述、音频信号、姿态序列，确保与这些条件信号的时空对齐对于产生连贯和真实的人体视频至关重要。

### **相关工作**

研究相关工作有：变分自编码器（VAE）、生成对抗网络（GAN）和扩散模型在图像和视频生成中的应用，以及之前关于视频生成和运动生成的调查。

![image-20241205165914385](https://raw.githubusercontent.com/chongzicbo/images/main/picgo/image-20241205165914385.png)

# **2.数据集和评价指标**

### **2.1 评价指标**

![image-20241205165950954](https://raw.githubusercontent.com/chongzicbo/images/main/picgo/image-20241205165950954.png)

在这个领域对生成的人类视频进行评估涉及几个关键方面：图像质量、视频质量、一致性、多样性、美学以及动作准确性。这些类别对于全面评估不同方法的性能至关重要。

- 图像质量：关注单个帧的视觉保真度，评估像素级差异、结构相似性和感知相似性，以确保帧与真实帧紧密匹配。
- 视频质量：将此评估扩展到时间领域，评估帧序列的一致性和现实感，以捕捉现实世界动作的动态特性。
- 时间一致性：确保生成的内容在时间上保持自然的流动和同步，这对于涉及同步音频和视频的应用至关重要。
- 多样性评估：生成内容的多样性和丰富性，确保模型能够产生广泛范围的现实视频。
- 动作准确性：评估视频中人类动作和运动的精确度，这对于这些动作的正确性至关重要的应用至关重要。这些指标共同提供了一个全面的框架，用于评估人类视频生成方法的表现和质量。

### **2.2 数据集**

![image-20241205170055400](https://raw.githubusercontent.com/chongzicbo/images/main/picgo/image-20241205170055400.png)

对于视频生成任务，有效表示视频中的姿态和运动信息至关重要。这里将介绍常见的姿态注释格式、它们的特点以及常用的方法。

- 2D姿态：使用关键点形成骨架图，用于识别和分析二维中的人类姿态。数据通常以每个关节的(x, y)坐标集合的形式呈现。常用方法：OpenPose[65]、DwPose[66]、PoseNet[67]、HRNet[68]、StackPose[69]。

- 3D姿态：为2D姿态增加深度，提供详细的3D坐标(x, y, z)。常用方法：ExPose[70]、Alphapose[71]、MotionBERT[72]。

- 3D网格(3D Mesh)：使用多边形网格来表示人类表面形状，以实现逼真的模型。数据格式通常包括网格的顶点和面。常用方法：SMPL[73]、SMPL-X[74]。
- 光流(Optical Flow)：代表像素的运动向量，用于描述视频中的运动方向和速度。数据通常存储为二维向量场。常见方法：MMFlow[75]、FlowNet[76]、RAFT[77]。
- 深度图(Depth)：创建一个深度图，显示每个像素与相机的距离，对于3D重建和AR很有用。数据通常是深度图像的形式，其中每个像素值对应于与相机的距离。常见方法：vid2depth[78]、monodepth2[79]、Depth Anything[80]。
- 密集姿态图(Dense pose)：将3D身体表面坐标映射到每个像素，以获取详细的姿态信息。数据包括每个像素的UV坐标，映射到一个3D身体模型。常见方法：DensePose[81]。

# **3.人类视频生成研究方法**

### **3.1 文本到人类视频生成**

![image-20241205170123022](https://raw.githubusercontent.com/chongzicbo/images/main/picgo/image-20241205170123022.png)

文本可以描述特定的外观、场景和风格，为生成模型提供丰富的信息源，以控制生成的内容。最近的一些生成方法，如稳定扩散[82]和Sora[14]，已经表明使用文本作为输入来生成图像和视频取得了令人印象深刻的结果。

然而，与专注于视频连贯性的通用视频生成任务不同，人类视频生成需要对人体的外观和运动进行精确控制。现有方法从两个主要角度应对这一挑战：使用文本来保持外观和从文本中提取语义信息以控制姿态。图3展示了现有文本驱动人视频生成研究的概述。

#### **文本驱动的人体外观控制**

为了控制生成视频中人体的外观，有两种方法：一种是直接提供参考图像，另一种是使用输入文本描述来控制生成的人体外观。在这里，我们讨论文本驱动的人体外观控制方法。为了确保生成视频中的外观与文本描述保持一致，同时在帧中保留身份细节，ID-Animator[1]利用一个预训练的文本到视频(T2V)模型和一个轻量级面部适配器来编码与身份相关的嵌入。文本描述指导生成人体视频，并控制视频中角色的外观。同样地，[2]使用文本描述来提供关于角色内容的语义信息，确保生成的视频与文本描述保持一致。

#### **文本驱动的人体运动控制**

现有方法精确控制生成视频中人体运动的典型方法遵循两种方法：

- 一种方法是遵循两阶段流程。它首先根据任务根据输入文本的语义生成相应的姿态，然后使用这些生成的姿态来指导运动。关于第二阶段姿态引导生成方法的更多细节可以在第六节中参考。对于这类任务，需要建立文本和姿态之间的联系来控制视频中的运动。HMTV[83]使用描述性文本生成初始的3D人形动作并控制摄像机角度，确保动态且逼真的视频输出。文本指导视频中的动作和摄像机移动，提供对角色动作和观众视角的精确控制。对于手语制作任务，SignSynth[84]使用Gloss2Pose网络生成手语姿势，并使用GAN创建高质量的手语视频。同样地，H-DNA[85]通过首先生成手势姿势，然后使用GAN产生最终视频，将口语句子翻译成手语视频。在SignLLM[86]中，文本描述被转换为gloss（一种中间手语表示），然后映射到姿势，这些姿势被渲染成手语视频。在这里，捕获文本的语义以与所描述的姿态对齐。在Cued Speech[87]、[88]生成任务中，[89]首先利用大型语言模型（LLM）将文本转换为描述性的gloss，然后使用gloss生成精细的姿势。

- 另一种方法直接使用文本作为提示来指导视频动作的生成。例如，Text2Performer[53]涉及运动文本和运动编码器。运动文本描述了运动，如“她向右摆动。”该模型通过分别表示外观和运动来隐式地模拟这些描述，从而生成具有统一外观和动作的高质量视频。

### **3.2 音频到人类视频生成**

两个主要子任务：基于语音驱动的人类视频和基于音乐驱动的人类视频。基于语音驱动的人类视频生成旨在根据输入的语音音频生成一系列人类手势，这要求生成的人类动作与音频在高级语义、情感和节奏方面都要和谐一致。而基于音乐驱动的人类视频生成则专注于合成一个人跳舞或演奏特定乐器所引导的视频，尤其是低级节拍对齐。在这种情况下，直接将音频转换为视频构成了一个复杂的挑战。以往的研究通常遵循两阶段流程，包括音频到运动和运动到视频，如图4所示。

![image-20241205170150968](https://raw.githubusercontent.com/chongzicbo/images/main/picgo/image-20241205170150968.png)

#### **语音驱动的人类视频生成**

许多现有工作集中在生成说话视频上，主要集中在头部区域[95]、[96]。相比之下，我们的综述关注的是包括身体动作[3]、[4]、[61]、[92]至[94]的作品。据我们所知，所有这些作品都属于共语音手势视频生成的领域。鉴于运动表示对于最终视频的重要性，我们从运动生成的视角回顾了这些作品。

在语音驱动的人体视频生成中，一些方法[61][92][93]从二维骨架[3][61]或三维模型[92][93]的序列合成说话视频，渲染过程与手势生成分开。然而，手工制作的结构性人体先验，如二维/三维骨架，完全丢弃了关键点周围的外观信息，使得精确的运动控制和视频渲染变得极具挑战性。此外，姿态估计器的预训练依赖于手工注释，导致错误积累，通常会导致抖动。为了缓解这些问题，ANGIE[62]利用一个无监督特征MRAA[64]来模拟身体运动。然后使用VQ-VAE[97]来量化常见模式，接着是一个类似GPT的网络来预测离散的运动模式以生成手势视频。然而，MRAA作为运动的粗略建模是线性的，无法表示复杂形状的区域，限制了ANGIE生成的姿态视频的质量。此外，直接将协方差与语音关联是不恰当的。为了解决这些挑战，DiffTED和He等人提出了一种解耦运动与手势视频的方法，同时保留身体区域的临界外观信息。他们使用学习到的薄板样条(TPS)运动模型的二维关键点[37]作为生成的目标，并利用TPS运动模型将关键点渲染成图像。此外，受到最近扩散模型成功的影响，DiffTED和He等人提出了一种基于扩散的方法来生成多样化的手势序列。

#### **音乐驱动的人体视频生成**

音乐驱动的人体视频生成独特地结合了运动合成和音乐解释，旨在创建与输入音乐节奏同步的人类动作。这超出了普通运动合成的范畴，因为与节拍对齐的动作很复杂[90]。我们探索了两个子任务，即音乐到舞蹈和音乐到表演。

为了实现节拍感知的运动生成，一些音乐到舞蹈的视频生成工作[6],[90]明确地从音乐音频中检测节拍，或者设计一个匹配相位来学习这两种不同模态之间的关系[47]。伊斯兰·阿尔·[6]首先执行从输入音乐中检测节拍和重复模式提取，然后生成一个人跳舞的数学模型，并将其转换为目标人物的现实图像。Dabfusion[90]应用节拍提取器来明确地将节拍特征从音乐中分离出来。然后使用这些节拍特征来指导潜在光流的生产，接着进行反向流估计以生成输出视频。不同之处在于，DanceIt[47]在第一个匹配阶段学习这两种不同模态之间的关系，然后在生成阶段检索每个音乐音频的一系列姿态片段并进行空间-时间对齐。

对于音乐到表演的视频生成，从低维音频模态生成高维时间一致的视频是具有挑战性的。朱等人[5] 提出了一个多阶段框架，首先从给定的音频生成粗略的视频，然后通过整合预测关键点的帧内结构信息和时间信息来进行精细化处理，最终生成性能视频。Music2Play[91] 以自回归的方式获得一系列姿态，并估计从一对姿态中得到的密集流场信息，最后融合多模态信息（音频、流和图像）来合成输出帧。

### **3.3 姿势到人类视频生成**

![image-20241205170214205](https://raw.githubusercontent.com/chongzicbo/images/main/picgo/image-20241205170214205.png)

如图5所示，现有的基于姿态驱动的人类视频生成研究通常遵循一个共同的流程。在姿态驱动的人类视频生成任务中，各种姿态类型，包括骨架姿态、密集姿态、深度、网格和光流（如表IV所示），作为常见的指导模态，与传统文本和语音输入一起使用。根据条件姿态的数量，我们可以将现有的姿态引导的人类视频生成方法分为两类。第一类只使用单一类型的姿态，这些姿态被记录为单条件姿态引导方法。第二类使用不同类型的姿态信号，这些信号被称为多条件姿态引导方法。

![image-20241205170230215](https://raw.githubusercontent.com/chongzicbo/images/main/picgo/image-20241205170230215.png)

#### **单条件姿态引导方法**

在所有类型的条件信号中，最常见的有骨架姿态和密集姿态。早期的基于姿态引导的人类视频生成方法[26]、[52]、[75]、[99]-[106]主要利用条件对抗网络，如CGAN[107]、pix2pix[108]和pix2pixHD[109]。这些方法使用OpenPose[65]或StackPose[69]方法提取骨架姿态，或者使用DensePose方法提取密集姿态，并将提取的骨架姿态或密集姿态作为条件信号输入到CGAN或pix2pix生成模型中。

随着条件生成模型的发展，当前的方法[8],[29],[110]-[112]主要利用稳定扩散(SD)[113]或稳定视频扩散(SVD)[114]。

[115]作为视频生成模型的骨干。具体来说，MagicPose[29]通过ControlNet[116]将姿态特征注入扩散模型中。与直接使用ControlNet相比，如MotionFollower[110]、MimicMotion[111]、AnimateAnyone[8]和UniAnimate[112]等，这些方法使用Dw-Pose[117]或OpenPose[65]从targvideo帧中提取骨架姿态。为了将提取的骨架姿态与潜在空间中的噪声对齐，并在去噪处理过程中有效利用姿态引导，它们设计了轻量级神经网络（仅由几个卷积层组成），作为姿态引导器。

与上述骨架姿态引导的视频生成扩散模型不同，如DreamPose[118]和MagicAnimate[119]等，这些方法利用DensePose[81]方法提取密集姿态，并通过ControlNet直接将密集姿态和噪声连接起来，形成去噪UNet。与这些类型的二维姿态（骨架姿态和密集姿态）不同，Human4DiT[120]使用SMPL[121]提取相应的三维网格图。受到Sora和其他变体[14],[122]的工作启发，Human4DiT[120]将扩散变压器视为视频生成的骨干。

#### **多条件姿态引导方法**

除了基于单一条件的姿态引导人体视频生成外，SD[113]和SVD[114],[115]的最新成功为多条件姿态引导人体视频生成奠定了基础。大多数现有的姿态引导方法使用骨架姿态或密集姿态作为条件输入。然而，这些单条件姿态引导方法往往对复杂背景的泛化能力较差，并且在不同身体和同一个体的不同部分之间存在遮挡问题。

为了应对考虑复杂背景的泛化能力差的问题，DISCO[48]提出了一种创新的模型架构，该架构在背景和骨架姿态上实现了解耦控制，从而提高了舞蹈生成的连贯性。这种架构能够整合来自不同来源的所见和新颖的主题、背景和姿态。Follow Your Pose v2[138]通过与其他条件引导器集成光流指导，增强了背景稳定性。Liu等人[125]分离了前景和背景的运动表示，在使用稀疏跟踪点建模背景运动的同时，通过基于姿态的运动动画人物，捕捉人物活动与环境变化之间的自然交互。

为了解决遮挡问题，Follow-Your-Pose v2[138]使用深度引导器解决了多角色动画中的遮挡问题，并通过参考姿态引导器改进了角色外观学习。VividPose[98]引入了深度和网格信息，特别是与SMPL-X[137]模型结合使用时，有助于系统更好地处理遮挡和常见于人体姿态序列的复杂运动。DreaMoving[131]整合了深度信息和骨架姿态，帮助模型理解身体不同部分与环境之间的空间关系。深度信息对于处理遮挡很有用，因为它允许模型确定哪些身体部位在其他人前面或后面。

# **4.关键挑战**

1）遮挡问题。在收集的视频中，常见的情况是重叠的身体部位或多个人物遮挡，但大多数模型无法很好地处理相互影响的问题[98]、[138]。

2）身体变形。确保生成的视频特征如身体形状、面部和手部符合典型的人类特征是这项任务中的一个重大障碍。这个问题的一个常见例子是畸形手的出现[139]。

3）外观不一致性。生成人类视频还需要确保生成视频中的人类的各种特征，包括面部、身体、服装、配饰等，保持一致。然而，大多数模型都无法实现完全令人满意的统一性。

4）背景影响。当在前景中生成包含人体视频时，背景的一致性和与前景人体的和谐也是主要挑战。背景控制不佳将影响人类生成的质量，并带来额外的抖动和失真。

5）时间不对齐。在由时间信号指导的模型中，特别是音频到人视频生成模型，嘴唇和声音的对齐是一个显著挑战，以提高质量。

6）不自然的姿态。当前生成的人类视频常常存在不自然的姿态问题。这个问题的具体表现包括生成的视频与输入的引导姿态之间的一致性问题，以及生成视频中动作的自然性。

除了上述代表性挑战外，在文本或音频驱动模型中，由于数据集中的多对一映射特性，即单个输入文本或音频可以对应多个有效的输出，因此尝试直接将输入与单一“正确”的手势匹配可能会导致不可靠且有偏见的关联。这种方法阻碍了模型捕捉和学习数据中存在的变异的能力。

# **5.影响人类视频生成质量的因素**

- 生成范式。与基于姿态的方法（可以被视为单阶段方法）相比，文本和音频驱动的方法可以分为单阶段和双阶段方法。前者直接使用输入文本或音频作为提示来指导人类视频生成，而后者从输入文本或音频生成姿态，然后使用这些生成的姿态作为信号来指导人类视频生成。在双阶段方法中引入各种姿态类型，如骨架姿态，提供了额外的几何和语义信息，增强了视频动作的准确性和真实性。这使得双阶段方法显著比单阶段方法更有效，尽管代价是某些效率的损失。
- 骨干网络。扩散模型，如SD和SVD，在包括人类视频生成在内的各种生成任务中被广泛使用，因为它们具有卓越的性能和多样性。然而，与GANs不同，GANs在单个采样步骤中生成样本，扩散模型需要多个采样步骤，从而增加了训练和推理的时间成本。
- 条件姿态。不同类型的条件姿态之所以有效，是因为它们提供了互补的信息。最常见的骨架姿态能够准确地描述帧中人体空间信息和身体部位之间的相对位置。然而，它捕捉的是离散的姿态变化而不是连续的运动细节，提供了有限的时间连贯性。相比之下，光流本质上包含了时间信息，捕捉连续帧之间的变化，并在特征空间中提供连续的运动轨迹。这使得模型能够生成具有帧与帧之间平滑过渡的视频，避免跳跃或不连续性。此外，骨架姿态不包括背景和细节建模，而深度图则捕捉人体与背景之间的距离信息，以及表面细节和深度变化。3D网格提供了物体表面的详细几何结构，这是骨架姿态所缺乏的。总之，不同类型的姿态提供了互补的时空信息，没有一种统一的姿态类型能满足所有要求。不同的场景和问题可能需要不同的姿态。

# **6.未来工作**

-  大规模高质量的人类视频数据集。现有的公共数据集，包括人类动作和人类舞蹈领域的数据集，在规模上相对较小。收集高质量的人类视频数据集既具有挑战性又昂贵。然而，大规模、高质量的人类视频数据集对于开发人类视频生成的基础模型至关重要。
- 长视频生成。当前的人类视频生成方法通常只能产生几秒钟的视频。生成持续几分钟甚至几小时的视频是一个重大挑战。因此，未来的研究应该专注于长时人类视频的生成。
- 逼真视频生成。如前所述，遮挡、身体变形、姿态不自然和外观不一致等问题可能导致低质量的视频生成。解决这些视觉和美学问题以确保生成的人体运动遵循现实世界的物理定律是一个主要挑战。创建具有高度逼真视觉效果的视频仍然是一项艰巨的任务。
- 人类视频扩散效率。扩散模型已成为人类视频生成任务的核心。然而，视频扩散模型的高昂训练成本和部署要求构成了重大挑战。降低训练成本和缩小模型规模是关键问题。因此，探索视频扩散模型的效率对于未来研究来说是一个有价值的方向。
- 细粒度可控性。现有的多模态驱动的人类视频生成方法，即使结合了额外的条件信号，如3D网格和深度图以及骨架姿态，仍然缺乏对特定身体部位，特别是手和脸的细粒度控制。未来的研究可以专注于实现这些详细人体区域的细粒度、可控生成。
- 交互性。除了探索细粒度可控性，未来的工作还可以进一步研究交互可控性。这将允许用户通过简单的动作（如点击）操纵元素，如手臂运动或面部表情，最终生成满足用户满意度的真人视频。