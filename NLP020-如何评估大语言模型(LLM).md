# 介绍

随着chatgpt和其它大语言模型的发布，模型的数量在不断的增加。新的语言模型每天都在发布。尽管如此，任然没有固定的或者标准化的方法来评估这些大语言模型的质量。本文将回顾现有的大语言模型(LLM)和基于LLM的系统的评估框架。此外，还将尝试分析LLM应该根据哪些因素来进行评估。

![LLM](https://raw.githubusercontent.com/chongzicbo/images/main/picgo/LLM.png)

# 为什么需要一个综合的评估框架

在技术发展的早期阶段，更容易确定哪些领域需要改进。然而，随着技术的进步和新的替代品的出现，越来越难以确定哪种选择是最好的。这就需要有一个可靠的评价框架，能够准确地判断LLM的质量。

就LLM而言，对真实评估框架的迫切需求变得更加重要。这样的框架可以通过以下三种方式用于评估LLM：

- 一个适当的框架将有助于相关机构评估模型的安全性、准确性、可靠性或可用性问题。
- 目前，大型科技公司似乎在盲目竞相推出LLM，许多公司只是在产品上加上免责声明，以免除自己的责任。制定一个全面的评估框架将有助于利益攸关方更负责任地发布这些模型。
- 一个全面地评估框架也将有助于帮助这些LLM的用户确定在何处以及如何对这些模型进行微调，以及使用哪些额外数据来实现部署。

# LLM的现有评估框架

评估大模型以确定其在各种应用程序中的质量和有用性是至关重要的。目前已经有了几个框架来评估LLM，但没有一个框架足够全面，足以涵盖语言理解的各个方面。让我们来看看现有的一些主要评估框架。

|               框架名称               |                        评估考虑的因素                        |                            链接                            |
| :----------------------------------: | :----------------------------------------------------------: | :--------------------------------------------------------: |
|              Big Bench               |                           泛化能力                           |            https://github.com/google/BIG-bench             |
|            GLUE Benchmark            |        语法、释义、文本相似、推理、文本蕴含、指代消解        |                 https://gluebenchmark.com/                 |
|         SuperGLUE Benchmark          | 自然语言理解、推理、理解训练数据之外的复杂语句、自然语言生成、对话、常识推理、信息检索、阅读理解 |              https://super.gluebenchmark.com/              |
|        OpenAI Moderation API         |                  过滤掉有害或者不安全的内容                  | https://platform.openai.com/docs/api-reference/moderations |
|                 MMLU                 |                  跨不同任务和领域的语言理解                  |             https://github.com/hendrycks/test              |
|          EleutherAI LM Eval          |        少样本评估和在一系列任务上使用最少的微调的性能        |    https://github.com/EleutherAI/lm-evaluation-harness     |
|             OpenAI Evals             | 文本生成的准确性、多样性、一致性、稳健性、可转移性、效率和公平性 |              https://github.com/openai/evals               |
|        Adversarial NLI (ANLI)        | 鲁棒性、泛化、推理的连贯解释、相似例子之间推理的一致性、资源使用方面的效率（内存使用、推理时间和训练时间） |          https://github.com/facebookresearch/anli          |
| LIT (Language Interpretability Tool) |   用户定义指标评估平台。深入了解他们的优势、劣势和潜在偏见   |              https://pair-code.github.io/lit/              |
|                ParlAI                | 准确性、F1、困惑度、对相关性、流利性和连贯性等标准的人工评估、速度和资源利用率、鲁棒性（这评估模型在不同条件下的表现，如噪声输入、对抗性攻击或不同数据质量水平）、泛化 |         https://github.com/facebookresearch/ParlAI         |
|                 CoQA                 |     理解一段文字，回答对话中出现的一系列相互关联的问题。     |            https://stanfordnlp.github.io/coqa/             |
|               LAMBADA                |          使用对文章最后一个单词的预测进行长期理解。          |       https://zenodo.org/record/2630551#.ZFUKS-zML0p       |
|              HellaSwag               |                           推理能力                           |            https://rowanzellers.com/hellaswag/             |
|                LogiQA                |                         逻辑推理能力                         |          https://github.com/lgw863/LogiQA-dataset          |
|               MultiNLI               |                  理解不同文体句子之间的关系                  |          https://cims.nyu.edu/~sbowman/multinli/           |
|                SQUAD                 |                         阅读理解任务                         |        https://rajpurkar.github.io/SQuAD-explorer/         |

# 现存推理框架存在的问题

以上评估大型语言模型的方法各有其优点。然而上述似乎都不充分

- 上述框架均未将安全性视为评估因素。尽管“OpenAI Moderation API”在一定程度上解决了这一问题，但这还不够。
- 上述框架在评估模型的因素方面是分散的。它们都不够全面，无法自给自足。



# 评估LLM时应该考虑什么因素

在回顾了现有的评估框架后，下一步是确定在评估大型语言模型（LLM）的质量时应该考虑哪些因素。我们对12名数据科学专业人士进行了一项调查。这些人对LLM的工作方式和他们能做什么有着相当的了解。他们还尝试并测试了多种LLM。这项调查旨在根据他们的理解列出所有重要因素，他们据此判断LLM的质量。

最后，我们发现有几个关键因素需要考虑：

1.真实性

LLM生成的结果的准确性至关重要。这包括事实的正确性，以及推论和解决方案的准确性。

2.速度

模型产生结果的速度很重要，尤其是当需要为重要应用部署时。虽然在某些情况下，较慢的模型可能是可以接受的，但对速度要求较高的应用需要较快的模型。

3.语法和可读性

LLM必须以可读的格式生成语言。确保正确的语法和句子结构是至关重要的。

4.无偏见

LLM不受与性别、种族和其他因素有关的社会偏见的影响，这一点至关重要。

5.回溯

知道模型推断的来源对于人类重新检查其基础是必要的。如果没有这一点，LLM的性能仍然是一个黑匣子。

6.安全与责任

人工智能模型的护栏是必要的。尽管公司正在努力确保这些应对措施的安全性，但仍有很大的改进空间。

7.理解上下文

当人类向人工智能聊天机器人咨询有关其一般和个人生活的建议时，该模型根据特定条件提供更好的解决方案是很重要的。同一个问题在不同的背景下提出，可能会有不同的答案。

8.文本操作

LLM应该能够执行基本的文本操作，如文本分类、翻译、摘要等。

9.IQ

智商是一种用于判断人类智力的指标，也可以应用于机器。

10.EQ

情商是人类智力的另一个方面，可以应用于LLM。EQ更高的型号使用起来更安全。

11.通用性

模型可以涵盖的领域和语言的数量是另一个需要考虑的重要因素。它可以用于将模型分类为通用人工智能或特定于给定领域的人工智能。

12.实时更新

利用最新信息更新的系统可以做出更广泛的贡献并产生更好的结果。

13.成本

还应考虑开发和运营成本。

14.一致性

相同或相似的提示应该产生相同或几乎相同的响应，否则很难确保商业部署的质量。

15.提示工程的范围

获得最佳响应所需的详细和结构化的提示工程水平也可以用于比较两个模型。

# 结论

大型语言模型（LLM）的发展使自然语言处理领域发生了革命性的变化。然而，仍然需要一个全面和标准化的LLM评估框架来评估这些模型的质量。现有的框架提供了宝贵的见解，但缺乏全面性和标准化，没有将安全性视为评估的一个因素。

一个可靠的评估框架应该考虑真实性、速度、语法和可读性、无偏性、回溯性、安全性、理解上下文、文本操作、智商、情商、多功能性和实时更新等因素。开发这样一个框架将有助于利益相关者负责任地发布LLM，并确保其质量、可用性和安全性。与相关机构和专家合作对于建立一个真实和全面的LLM评估框架是必要的。



> 来源：[How to Evaluate a Large Language Model - Evaluate LLMs (analyticsvidhya.com)](https://www.analyticsvidhya.com/blog/2023/05/how-to-evaluate-a-large-language-model-llm/)

