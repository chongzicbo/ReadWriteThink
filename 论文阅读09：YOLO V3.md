# 摘要

我们对YOLO做了一些更新，做了一些小小的更改使它变得更好。我们还训练了更大的网络，比之前的网络更精确，而且还是运行的很快。对于$320 \times 320$的图像，花费22ms，mAP为28.2，与SSD一样准确，但速度快三倍。当我们看到旧的0.5 IOU mAP检测指标YOLOv3是相当好的。在Titan X上，它在51毫秒内达到57.9 AP50，相比之下，RetinaNet在198毫秒内达到了57.5 $AP_{50}$，性能相似，但速度快3.8倍。代码见https://pjreddie.com/yolo/.

# 引言



# YOLO V3

## 边界框预测(Bounding Box Prediction)

跟YOLO9000一样，我们的系统使用维度聚类作为锚框来预测边界框。

网络为每个边界框预测4个坐标，$t_x，t_y，t_w，t_h$。如果单元格从图像的左上角偏移$（c_x，c_y）$，并且边界框先验具有宽度和高度$p_w，p_h$，则预测对应于：
$$
b_{x}=\sigma(t_{x})+c_{x}
$$

$$
b_{y}=\sigma(t_{y})+c_{y}
$$

$$
b_w=p_w e^{t_w}
$$

$$
b_h=p_h e^{t_h}
$$

在训练过程中，我们使用误差平方和损失。如果某个坐标预测的真实值是$\hat{t}_*$，则我们的梯度是真实值（从地面实况框计算）减去我们的预测：$\hat{t}_* -t_*$。

YOLOv3使用逻辑回归预测每个边界框的目标分数（objectness score）。如果真实标签框与某个边界框重叠的面积比与其他任何边界框都大，那么这个先验边界框为1。按照[15]的做法，如果先验边界框不是最好的，但是确实与目标的真实标签框重叠的面积大于阈值，我们就会忽略这个预测。我们使用阈值为0.5。与[15]不同，我们的系统只为每个真实标签框分配一个边界框。如果先验边界框未分配给真实标签框，则不会产生坐标或类别预测损失，只会产生目标预测损失。

![image-20230919220635760](https://raw.githubusercontent.com/chongzicbo/images/main/picgo/image-20230919220635760.png)

## 分类预测

每个边界框都会使用多标记分类来预测框中可能包含的类。我们不用softmax，而是用单独的逻辑分类器，因为我们发现前者对于提升网络性能没什么作用。在训练过程中，我们用二元交叉熵损失来预测类别。

这个规划有助于我们把YOLO用于更复杂的领域，如开放的图像数据集。这个数据集中包含了大量重叠的标签（如女性和人）。如果我们用的是softmax，它会强加一个假设，使得每个框只包含一个类别。但通常情况下这样的做法是不妥的，相比之下，多标记的分类方法能更好的模拟数据。

## 跨尺度预测

YOLOv3预测3种不同尺度的框。 我们的系统使用类似金字塔网络的相似概念，从这些尺度中提取特征[6]。 在我们的基本特征提取器上添加几个卷积层，其中最后一个卷积层预测了一个三维张量——边界框，目标和类别预测。 在我们的COCO实验[8]中，我们为每个尺度预测3个框，所以对于4个边界框偏移量，1个目标预测和80个类别预测，张量的大小为N×N×[3 *（4 + 1 + 80）]。

接下来，我们从前面的2个层中取得特征图，并将其上采样2倍。我们还从网络中的较前的层中获取特征图，并使用按元素相加的方式将其与我们的上采样特征图进行合并。这种方法使我们能够从上采样的特征图中获得更有意义的语义信息，同时可以从更前的层中获取更细粒度的信息。然后，我们再添加几个卷积层来处理这个组合的特征图，并最终预测出一个类似的张量，虽然其尺寸是之前的两倍。

我们再次使用相同的设计来预测最终尺寸的边界框。因此，第三个尺寸的预测将既能从所有先前的计算，又能从网络前面的层中的细粒度的特征中获益。

我们仍然使用k-means聚类来确定我们的先验边界框。我们只是选择了9个类和3个尺度，然后在所有尺度上均匀分割聚类。在COCO数据集上，9个聚类分别为（10×13），（16×30），（33×23），（30×61），（62×45），（59×119），（116×90） ，（156×198），（373×326）。

## 特征提取器

我们使用新的网络来执行特征提取。我们的新网络融合了YOLOv2，Darknet-19和新颖的残差网络的思想。我们的网络使用连续的3×3和1×1卷积层，但现在多了一些捷径连接（shortcut connetction），而且规模更大。它有53个卷积层，所以我们称之为....emmm...... Darknet-53！

![image-20230921222709689](https://raw.githubusercontent.com/chongzicbo/images/main/picgo/image-20230921222709689.png)

这个新网络比Darknet-19功能强大得多，但仍比ResNet-101或ResNet-152更高效。 以下是一些ImageNet结果：

![image-20230921223339848](C:\Users\chengbo\AppData\Roaming\Typora\typora-user-images\image-20230921223339848.png)

每个网络都使用相同的设置进行训练，并在256×256的图像上进行单精度测试。 运行时间是在Titan X上用256×256图像进行测量的。因此，Darknet-53可与最先进的分类器相媲美，但浮点运算更少，速度更快。 Darknet-53比ResNet-101更好，且速度快1.5倍。 Darknet-53与ResNet-152具有相似的性能，但速度快2倍。

Darknet-53也实现了最高的每秒浮点运算测量。 这意味着网络结构可以更好地利用GPU，使它的评测更加高效，更快。 这主要是因为ResNets的层数太多，效率不高。

## 与其他方法的对比

![image-20230921223945236](C:\Users\chengbo\AppData\Roaming\Typora\typora-user-images\image-20230921223945236.png)

![image-20230921224122003](C:\Users\chengbo\AppData\Roaming\Typora\typora-user-images\image-20230921224122003.png)

## 我们试过但不凑效的方法

我们在研究YOLOv3时尝试了很多东西，但很多都不起作用。 这是我们要记住的血的教训。

**锚框的x，y偏移预测**。 我们尝试使用常规的锚框预测机制，比如利用线性激活将坐标x、y的偏移程度预测为边界框宽度或高度的倍数。但我们发现这种方法降低了模型的稳定性，并且效果不佳。

**用线性激活代替逻辑激活进行x，y预测**。 我们尝试使用线性激活代替逻辑激活来直接预测x，y偏移。 这导致了MAP下降了几个点。

**focal loss**。 我们尝试使用focal loss。 它使得平均分下降2个点。 YOLOv3可能已经对focal loss试图解决的问题具有鲁棒性，因为它具有单独的目标预测和条件类别预测。 因此，对于大多数例子来说，类别预测没有损失？ 或者其他的东西？ 我们并不完全确定。

**双IOU阈值和真值分配**。 Faster R-CNN在训练期间使用两个IOU阈值。 如果一个预测与真实标签框重叠超过0.7，它就是一个正的样本，若重叠为[0.3，0.7]之间，那么它会被忽略，若它与所有的真实标签框的IOU小于.3，那么这是一个负样本。 我们尝试了类似的策略，但无法取得好的结果。



[YOLO v3 论文翻译 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/37201615)